"""
Generic Hierarchical Chunking Strategy
Uses LLM to find natural document sections like table of contents
"""

import json
from typing import List, Dict, Tuple
from .base import SemanticChunkingStrategy, SemanticChunkingConfig, TextChunk
from ..domains import BaseNER


class HierarchicalChunker(SemanticChunkingStrategy):
    """
    Generic hierarchical chunking using LLM to find natural document sections
    """
    
    def __init__(self, config: SemanticChunkingConfig):
        super().__init__(config)
        self.llm_client = None
    
    def chunk(self, text: str, domains: List[BaseNER] = None) -> List[TextChunk]:
        """Chunk text using LLM-guided section detection"""
        
        print(f"üèõÔ∏è HIERARCHICAL: Starting analysis for {len(text):,} chars")
        
        if not self.llm_client:
            self._initialize_llm()
        
        # Get sections from LLM
        sections = self._find_sections(text)
        
        if not sections:
            print(f"‚ö†Ô∏è HIERARCHICAL: No sections found, fallback to single chunk")
            return [TextChunk(0, 0, len(text), text)]
        
        # Create chunks from sections
        chunks = self._create_chunks_from_sections(text, sections)
        
        print(f"‚úÖ HIERARCHICAL: Created {len(chunks)} chunks")
        return chunks
    
    def _initialize_llm(self):
        """Initialize LLM client"""
        try:
            from llm import LLMClient, Models
            self.llm_client = LLMClient(Models.GPT_4_1_NANO)
            print(f"ü§ñ HIERARCHICAL: Initialized LLM {Models.GPT_4_1_NANO}")
        except Exception as e:
            print(f"‚ùå HIERARCHICAL: Failed to initialize LLM: {e}")
            self.llm_client = None
    
    def _find_sections(self, text: str) -> List[Dict[str, str]]:
        """Find document sections using LLM"""
    
        prompt = f"""Jeste≈õ algorytmem sekwencyjnego dzielenia tekst√≥w na bloki tematyczne.

Twoja rola: Analizujesz dokument od poczƒÖtku do ko≈Ñca i wyznaczasz naturalne punkty podzia≈Çu na sekcje, jak g≈Çowica czytajƒÖca ta≈õmƒô.

Dzia≈Çasz jak parser - idziesz linia po linii i znajdujesz miejsca gdzie jeden temat siƒô ko≈Ñczy a nowy zaczyna.

TEKST DO PODZIELENIA:
{text}

ALGORYTM:
1. Zacznij od poczƒÖtku tekstu
2. Id≈∫ sekwencyjnie w d√≥≈Ç szukajƒÖc naturalnych granic tematycznych  
3. Ka≈ºda granica to poczƒÖtek nowej sekcji
4. Nie wracaj do ty≈Çu, nie pomijaj fragment√≥w
5. Ka≈ºda sekcja ko≈Ñczy siƒô tam gdzie zaczyna nastƒôpna

My≈õl jak spis tre≈õci - ka≈ºda pozycja to logiczna czƒô≈õƒá dokumentu w naturalnej kolejno≈õci.

JSON (sekcje w kolejno≈õci wystƒÖpienia):
{{
  "sections": [
    {{
      "start_text": "fragment poczƒÖtku pierwszej sekcji (unikalny w dokumencie)"
    }},
    {{
      "start_text": "fragment poczƒÖtku drugiej sekcji (dalej ni≈º pierwsza)"
    }}
  ]
}}

BƒÖd≈∫ jak parser: sekwencyjnie, bez skok√≥w, bez powt√≥rek."""
        
        try:
            from llm import LLMConfig
            config = LLMConfig(temperature=0.0)
            response = self.llm_client.chat(prompt, config)
            
            # Parse JSON response
            clean_response = response.strip()
            if '```json' in clean_response:
                clean_response = clean_response.split('```json')[1].split('```')[0]
            elif '```' in clean_response:
                parts = clean_response.split('```')
                if len(parts) >= 3:
                    clean_response = parts[1]
            
            data = json.loads(clean_response.strip())
            sections = data.get('sections', [])
            
            print(f"üéØ HIERARCHICAL: Found {len(sections)} sections")
            return sections
            
        except Exception as e:
            print(f"‚ö†Ô∏è HIERARCHICAL: Section detection failed: {e}")
            return []
    
    def _create_chunks_from_sections(self, text: str, sections: List[Dict[str, str]]) -> List[TextChunk]:
        """Create chunks from detected sections - sequential, no overlaps"""

        if not sections:
            return [TextChunk(0, 0, len(text), text)]

        # Find all section positions
        boundaries = []
        for section in sections:
            start_text = section.get('start_text', '')
            if not start_text:
                continue
            
            pos = text.find(start_text)
            if pos != -1:
                boundaries.append((pos, section))
                print(f"   üéØ Found section at {pos}: {start_text[:50]}...")

        if not boundaries:
            print(f"   ‚ö†Ô∏è No valid boundaries found")
            return [TextChunk(0, 0, len(text), text)]

        # Sort by position (sequential order)
        boundaries.sort(key=lambda x: x[0])

        # Remove duplicates (same position)
        unique_boundaries = []
        used_positions = set()
        for pos, section in boundaries:
            if pos not in used_positions:
                unique_boundaries.append((pos, section))
                used_positions.add(pos)

        print(f"   üìã Creating {len(unique_boundaries)} sequential chunks")

        # Create sequential chunks
        chunks = []
        for i, (start_pos, section) in enumerate(unique_boundaries):
            # End position is next section's start or document end
            if i + 1 < len(unique_boundaries):
                end_pos = unique_boundaries[i + 1][0]
            else:
                end_pos = len(text)
            
            # Extract chunk text
            chunk_text = text[start_pos:end_pos].strip()
            
            # Create chunk if large enough
            if len(chunk_text) >= self.config.min_chunk_size:
                chunk = TextChunk(
                    id=len(chunks),
                    start=start_pos,
                    end=end_pos,
                    text=chunk_text
                )
                chunks.append(chunk)
                print(f"   ‚úÖ Chunk {len(chunks)-1}: {len(chunk_text):,} chars ({start_pos}-{end_pos})")
            else:
                print(f"   ‚ö†Ô∏è Skipped small chunk: {len(chunk_text)} chars < {self.config.min_chunk_size}")

        return chunks if chunks else [TextChunk(0, 0, len(text), text)]
    
    def estimate_ram_usage(self, text_length: int) -> int:
        """Estimate RAM usage"""
        return 50 * 1024 * 1024  # 50MB estimate
    
    def get_strategy_info(self) -> dict:
        """Return strategy information"""
        return {
            "name": "hierarchical",
            "description": "LLM-guided document section detection",
            "features": ["Table of contents style chunking", "Natural document boundaries"]
        }