CEL - PIPE dla dokumentów typu OWU - skopiuje luigi_pipe
1. znaleźć w pdf spis tresci i odczytać z niego gdzie są strony podziału rozdziałów
a) poprosić szybki LLM o podanie punktów odciecia po numerze strony oraz semantycznie po frazie odcięcia start_text
b) pocięcie pdf na mniejsze chunki takie jak rozdziały z klasyfikacją co to za typ: spis tresci, definicje, rozdział, załącznik, tabela, przypisy
c) jesli brak spisu tresci - chunking strona po stronie i dalej jak do tej pory

AD. b) po odcieciu chunków każdy rodzaj chunku ma własny pipeline 
- definicje trzeba w najpierw wyjąć semantycznie z PDF tak żeby nie miały tresci które są po definicjach czyli np jakieś tresci rozdziałów
- rozdziały swój
- tabele swój
---------

docelowa architektura to Luigi robi to co do tej pory czyli tnie i tłumaczy na markdown, a potem wchodzi FAZA Postprocess czyli NER która:
1. tnie semantycznie całego zlepieńca markdown i sprawdza czy nie za długi a jak za długi to rekurencja na chunking semantyczny
2. jak czunki już są wystarczająco sensowne to uruchamia na każdym chunku domain clasifikator z dostępnych domen (EXCLUSION, DEFINITION,... GENERAL)
3. Uruchamia równolegle wszystkie czunki i dla każdego równolegle domeny (WOW)

Luigi Pipeline - HYBRID OPTIMAL:

Phase 1: DOCUMENT PROCESSING
├── FileDecisionTask
├── PDFToMarkdownTask 
└── MarkdownCleanupTask

Phase 2: INTELLIGENT NER
├── SemanticChunkingTask (recursive until optimal size)
├── ChunkDomainClassificationTask (EXCLUSION|DEFINITION|VARIANT|GENERAL)
└── MegaParallelNERTasks:
    ├── Chunk1 × [Domain1, Domain2, Domain3] → 3 parallel
    ├── Chunk2 × [Domain4] → 1 parallel  
    └── ChunkN × [Domain1, Domain5] → 2 parallel


---------------